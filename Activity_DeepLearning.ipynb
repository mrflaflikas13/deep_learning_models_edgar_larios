{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# For handling the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import metrics for regression \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Libraries needed for Streamlit app\n",
    "import nbformat\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6712 - loss: 0.8484 - val_accuracy: 0.9219 - val_loss: 0.2854\n",
      "Epoch 2/10\n",
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.2330 - val_accuracy: 0.9155 - val_loss: 0.2424\n",
      "Epoch 3/10\n",
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9560 - loss: 0.1585 - val_accuracy: 0.9415 - val_loss: 0.1994\n",
      "Epoch 4/10\n",
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9641 - loss: 0.1214 - val_accuracy: 0.9192 - val_loss: 0.2381\n",
      "Epoch 5/10\n",
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.0984 - val_accuracy: 0.9351 - val_loss: 0.2189\n",
      "Epoch 6/10\n",
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9767 - loss: 0.0774 - val_accuracy: 0.9623 - val_loss: 0.1605\n",
      "Epoch 7/10\n",
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9788 - loss: 0.0705 - val_accuracy: 0.9727 - val_loss: 0.1412\n",
      "Epoch 8/10\n",
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9828 - loss: 0.0559 - val_accuracy: 0.9507 - val_loss: 0.2158\n",
      "Epoch 9/10\n",
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9819 - loss: 0.0565 - val_accuracy: 0.9695 - val_loss: 0.1545\n",
      "Epoch 10/10\n",
      "\u001b[1m1135/1135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9850 - loss: 0.0484 - val_accuracy: 0.9405 - val_loss: 0.2264\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - accuracy: 0.9379 - loss: 0.2009\n",
      "Test Accuracy: 0.9406\n",
      "\u001b[1m316/316\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                 Books       0.99      0.83      0.90      2378\n",
      "Clothing & Accessories       0.97      0.98      0.98      1750\n",
      "           Electronics       0.97      0.95      0.96      2082\n",
      "             Household       0.89      0.99      0.93      3875\n",
      "\n",
      "              accuracy                           0.94     10085\n",
      "             macro avg       0.96      0.94      0.94     10085\n",
      "          weighted avg       0.95      0.94      0.94     10085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and clean the dataset\n",
    "df = pd.read_csv(\"ecommerceDataset.csv\", header=None)\n",
    "df.columns = [\"Category\", \"Text\"]\n",
    "\n",
    "# Drop any rows with missing data\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode text labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Label\"] = label_encoder.fit_transform(df[\"Category\"])\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Text\"], df[\"Label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize text\n",
    "max_vocab = 10000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Build model\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=max_vocab, output_dim=64, input_length=max_len),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(label_encoder.classes_), activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# ----------------------------------------------- Code needed to create the corresponding pickle files -----------------------------------------------\n",
    "\n",
    "# Save model\n",
    "model.save(\"model_text.keras\")\n",
    "\n",
    "# Save tokenizer\n",
    "with open(\"tokenizer_text.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save label encoder\n",
    "with open(\"label_encoder_text.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 1388529844224.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 1465060163584.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 1419510022144.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 1414898647040.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 1391384199168.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 1334003105792.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 1294334689280.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 1248728711168.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 1164456099840.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 1110408953856.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 1037230800896.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 906663559168.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 840978923520.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 753398972416.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 653280018432.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 586628530176.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 518765608960.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 445213900800.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 429388627968.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 374334652416.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 343062118400.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 332668272640.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 316685680640.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 311625908224.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 303493054464.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 298598105088.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - loss: 300489310208.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 287577735168.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 288492945408.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 296344125440.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 280623611904.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 298293428224.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - loss: 285721493504.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 289426079744.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - loss: 290296791040.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 287370313728.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 286144790528.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 284426764288.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 290841952256.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - loss: 291319152640.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 277734129664.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 284755689472.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 274185207808.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - loss: 283144650752.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 282178289664.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 281953009664.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 280288362496.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 281133678592.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 286177165312.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 277615902720.0000\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation metrics for the model:\n",
      "\n",
      "Root Mean Squared Error: 532464.14\n",
      "R^2 Score: 0.1167\n",
      "\n",
      "Predicted Weekly Sales for custom input: 1031579.69\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"Walmart_Sales.csv\")\n",
    "\n",
    "# Convert Date column to datetime format\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "# Extract useful features from Date\n",
    "data[\"Year\"] = data[\"Date\"].dt.year\n",
    "data[\"Month\"] = data[\"Date\"].dt.month\n",
    "data[\"Day\"] = data[\"Date\"].dt.day\n",
    "data[\"WeekOfYear\"] = data[\"Date\"].dt.isocalendar().week\n",
    "\n",
    "# Drop the original Date column\n",
    "data = data.drop(columns=[\"Date\"])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(columns=[\"Weekly_Sales\"])\n",
    "y = data[\"Weekly_Sales\"]\n",
    "\n",
    "# Normalize feature values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define a regression model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# Use the model \n",
    "custom_input = np.array([[1, 0, 45.0, 2.60, 211.0, 8.1, 2012, 11, 3, 44]]) # Construct a custom input in order to obtain the corresponding prediction value\n",
    "custom_input_scaled = scaler.transform(custom_input) # Normalize the custom input\n",
    "custom_prediction = model.predict(custom_input_scaled) # Obtain the predicted value that corresponds to the custom input\n",
    "\n",
    "# Show results\n",
    "print()\n",
    "print(\"Evaluation metrics for the model:\")\n",
    "print()\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")\n",
    "print()\n",
    "print(f\"Predicted Weekly Sales for custom input: {custom_prediction[0][0]:.2f}\") # Print the predicted value\n",
    "\n",
    "# ----------------------------------------- Code needed to create the corresponding pickle file -----------------------------------------\n",
    "\n",
    "# Save model\n",
    "model.save(\"model_regression.h5\")\n",
    "\n",
    "# Save scaler\n",
    "with open(\"scaler_regression.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.0405 - loss: 4.2606 - val_accuracy: 0.2392 - val_loss: 3.0320\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 105ms/step - accuracy: 0.3807 - loss: 2.3889 - val_accuracy: 0.3769 - val_loss: 2.4092\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.6799 - loss: 1.1841 - val_accuracy: 0.4292 - val_loss: 2.3588\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.8862 - loss: 0.4506 - val_accuracy: 0.3992 - val_loss: 2.9147\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.9669 - loss: 0.1478 - val_accuracy: 0.4131 - val_loss: 3.1355\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.9811 - loss: 0.0948 - val_accuracy: 0.4285 - val_loss: 3.5661\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 104ms/step - accuracy: 0.9912 - loss: 0.0621 - val_accuracy: 0.4392 - val_loss: 3.8038\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.9946 - loss: 0.0280 - val_accuracy: 0.4215 - val_loss: 4.0562\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 102ms/step - accuracy: 0.9935 - loss: 0.0339 - val_accuracy: 0.4085 - val_loss: 3.9321\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 103ms/step - accuracy: 0.9923 - loss: 0.0423 - val_accuracy: 0.4308 - val_loss: 3.9366\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4526 - loss: 3.9741\n",
      "\n",
      "Validation Accuracy of the model: 0.4308\n",
      "Validation Loss of the model: 3.9366\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      filename         predicted_label\n",
      "0  Image_1.jpg      CLODIUS PARNASSIAN\n",
      "1  Image_2.jpg           CRIMSON PATCH\n",
      "2  Image_3.jpg          ORANGE OAKLEAF\n",
      "3  Image_4.jpg             RED POSTMAN\n",
      "4  Image_5.jpg  MILBERTS TORTOISESHELL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load metadata\n",
    "train_df = pd.read_csv(\"Butterflies_Training_set.csv\")\n",
    "test_df = pd.read_csv(\"Butterflies_Testing_set.csv\")\n",
    "\n",
    "# Paths\n",
    "train_dir = \"train_images_butterflies\"  \n",
    "test_dir = \"test_images_butterflies\"\n",
    "\n",
    "# Parameters\n",
    "img_size = (128, 128)\n",
    "\n",
    "# Load and preprocess training images\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    img_path = os.path.join(train_dir, row[\"filename\"])\n",
    "    image = load_img(img_path, target_size=img_size)\n",
    "    image = img_to_array(image) / 255.0\n",
    "    X.append(image)\n",
    "    y.append(row[\"label\"])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print()\n",
    "print(f\"Validation Accuracy of the model: {val_acc:.4f}\")\n",
    "print(f\"Validation Loss of the model: {val_loss:.4f}\")\n",
    "\n",
    "# Predict on test set\n",
    "X_test = []\n",
    "\n",
    "for fname in test_df[\"filename\"]:\n",
    "    img_path = os.path.join(test_dir, fname)\n",
    "    image = load_img(img_path, target_size=img_size)\n",
    "    image = img_to_array(image) / 255.0\n",
    "    X_test.append(image)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "pred_probs = model.predict(X_test)\n",
    "pred_classes = np.argmax(pred_probs, axis=1)\n",
    "pred_labels = label_encoder.inverse_transform(pred_classes)\n",
    "\n",
    "# Save predictions\n",
    "test_df[\"predicted_label\"] = pred_labels\n",
    "test_df.to_csv(\"predicted_test_labels.csv\", index=False)\n",
    "print()\n",
    "print(test_df.head())\n",
    "\n",
    "# ------------------------------------------ Create the corresponding pickle file ------------------------------------------\n",
    "\n",
    "# Save model\n",
    "model.save(\"model_image.h5\")\n",
    "\n",
    "# Save label encoder\n",
    "with open(\"label_encoder_image.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
